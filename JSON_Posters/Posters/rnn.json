{
  "version": 2,
  "uid": "rnn",
  "front": {
    "title": "RNN",
    "chronology": {
      "epochStart": 1986,
      "epochEnd": 2014,
      "epochEvents": [
        {
          "year": 1986,
          "name": "First RNN Formulation"
        },
        {
          "year": 1990,
          "name": "Backpropagation Through Time"
        },
        {
          "year": 1997,
          "name": "Vanishing Gradient Problem Identified"
        },
        {
          "year": 2000,
          "name": "Echo State Networks"
        },
        {
          "year": 2008,
          "name": "Deep RNNs"
        }
      ]
    }
  },
  "back": {
    "layout": "auto",
    "text": "Recurrent Neural Networks (RNNs) represented a fundamental breakthrough in neural network architecture by introducing the ability to process sequential data through a form of memory. Unlike traditional feedforward networks, RNNs contain loops that allow information to persist, enabling them to use their internal state to process sequences of inputs. This made them particularly suitable for tasks where context and order matter, such as natural language processing, speech recognition, and time series prediction.\n\nThe key innovation of RNNs lies in their recurrent connections, where the output from a previous step is fed back as input to the current step. This recursive structure allows the network to maintain a form of memory about previous inputs in the sequence. However, basic RNNs suffered from the vanishing gradient problem, where they struggled to learn long-range dependencies in sequences. This limitation led to the development of more sophisticated architectures like LSTM and GRU networks. Despite being largely superseded by these advanced variants and eventually transformer models, traditional RNNs established the foundational concepts for sequence modeling that influenced all subsequent developments in the field."
  },
  "meta": {
    "modified": "2026-02-08T21:22:07.525Z",
    "categories": [
      "LLMmodels"
    ],
    "created": "2026-02-08T17:56:24.624Z"
  },
  "type": "poster-v2"
}