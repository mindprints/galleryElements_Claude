{
  "version": 2,
  "uid": "diffusion",
  "front": {
    "title": "Diffusion",
    "chronology": {
      "epochStart": 2015,
      "epochEnd": 2023,
      "epochEvents": [
        {
          "year": 2015,
          "name": "Early Noise-Based Generation Concepts"
        },
        {
          "year": 2020,
          "name": "DDPM (Denoising Diffusion Probabilistic Models)"
        },
        {
          "year": 2021,
          "name": "GLIDE and Guided Diffusion"
        },
        {
          "year": 2022,
          "name": "DALL-E 2 and Imagen"
        },
        {
          "year": 2022,
          "name": "Stable Diffusion Open Release"
        }
      ]
    }
  },
  "back": {
    "layout": "auto",
    "text": "Diffusion models represent a class of generative models that have emerged as powerful tools for creating high-quality images, audio, and other media. The core concept behind diffusion models involves a gradual process of adding noise to data and then learning to reverse this process, effectively training the model to transform noise into structured data. Unlike GANs, which rely on adversarial training, or VAEs, which directly encode data into a latent space, diffusion models operate by iteratively denoising data through a Markov chain of transformations.\n\nThe breakthrough of diffusion models lies in their combination of stable training dynamics with exceptional generation quality. Their formulation as a series of denoising steps makes them conceptually interpretable and mathematically elegant, with strong connections to non-equilibrium thermodynamics. Since their emergence in the deep learning landscape around 2020, diffusion models have produced state-of-the-art results in image generation, with models like DALL-E 2, Stable Diffusion, and Midjourney demonstrating unprecedented capabilities for text-to-image generation. Their impact extends beyond images to audio synthesis, 3D modeling, video generation, and molecule design. The diffusion approach has enabled remarkable advances in controllable generation, with models capable of following complex textual instructions to create highly detailed and coherent outputs."
  },
  "meta": {
    "modified": "2026-02-08T21:22:07.520Z",
    "categories": [
      "LLMmodels"
    ],
    "created": "2026-02-08T17:56:24.619Z"
  },
  "type": "poster-v2"
}