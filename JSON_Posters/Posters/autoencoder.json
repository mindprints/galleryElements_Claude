{
  "version": 2,
  "uid": "autoencoder",
  "front": {
    "title": "Autoencoder",
    "chronology": {
      "epochStart": 1986,
      "epochEnd": 2018,
      "epochEvents": [
        {
          "year": 1986,
          "name": "Early Autoencoder Concepts"
        },
        {
          "year": 2006,
          "name": "Deep Autoencoders"
        },
        {
          "year": 2008,
          "name": "Denoising Autoencoders"
        },
        {
          "year": 2011,
          "name": "Contractive Autoencoders"
        },
        {
          "year": 2014,
          "name": "Stacked Autoencoders"
        }
      ]
    }
  },
  "back": {
    "layout": "auto",
    "text": "Autoencoders represent a fundamental class of neural networks designed for unsupervised learning by compressing data into a lower-dimensional representation and then reconstructing it. The architecture consists of two primary components: an encoder that maps the input to a compact latent space (encoding), and a decoder that attempts to reconstruct the original input from this representation (decoding). Through this process of compression and reconstruction, autoencoders learn efficient, meaningful representations of data without requiring labels.\n\nThe key innovation of autoencoders lies in their ability to capture the most salient features of the input data in a compressed latent space. By constraining the encoding dimension to be smaller than the input, the network is forced to learn a compressed representation that preserves the most important aspects of the data. Various specialized autoencoder architectures have emerged, including denoising autoencoders (trained to recover clean data from corrupted inputs), sparse autoencoders (which impose sparsity constraints on the hidden layers), and deep autoencoders (with multiple layers in both encoder and decoder). Autoencoders have found applications in dimensionality reduction, anomaly detection, image denoising, feature learning, and as building blocks for more complex architectures like variational autoencoders."
  },
  "meta": {
    "modified": "2026-02-08T21:22:07.518Z",
    "categories": [
      "LLMmodels"
    ],
    "created": "2026-02-08T17:56:24.617Z"
  },
  "type": "poster-v2"
}