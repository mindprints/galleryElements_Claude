{
  "uid": "bert",
  "header": "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing pre-training developed by Google. Unlike previous models which read text input sequentially (left-to-right or right-to-left), BERT reads the entire sequence of words at once, allowing it to learn contextual relations between words in a text. This key innovation enabled BERT to achieve state-of-the-art results on a wide variety of NLP tasks when it was introduced in 2018.\n\nBERT's bidirectional approach represented a breakthrough in understanding context, as it allowed the model to consider both the left and right context of a word in all layers. This contextual understanding enabled significantly more nuanced language comprehension compared to previous unidirectional models. Pre-trained on a massive corpus of unlabeled text including the entire Wikipedia (2.5 billion words) and Book Corpus (800 million words), BERT uses a masked language model training objective where it randomly masks words in a sentence and then predicts them. BERT has been fine-tuned for specific tasks with additional output layers, showing remarkable performance in question answering, sentiment analysis, and other NLP applications.",
  "figure": "BERT",
  "chronology": {
    "epochStart": 2018,
    "epochEvents": [
      { "year": 2018, "name": "Initial Release" },
      { "year": 2018, "name": "Published by Google Research" },
      { "year": 2019, "name": "Integrated into Google Search" },
      { "year": 2019, "name": "RoBERTa refinement by Facebook" },
      { "year": 2019, "name": "DistilBERT lightweight version" }
    ],
    "epochEnd": 2020
  }
} 