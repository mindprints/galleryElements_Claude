{
  "version": 2,
  "uid": "lstm",
  "front": {
    "title": "LSTM",
    "chronology": {
      "epochStart": 1997,
      "epochEnd": 2018,
      "epochEvents": [
        {
          "year": 1997,
          "name": "Original LSTM Paper"
        },
        {
          "year": 2005,
          "name": "Bidirectional LSTM"
        },
        {
          "year": 2012,
          "name": "Deep LSTM Networks"
        },
        {
          "year": 2014,
          "name": "LSTM with Attention"
        },
        {
          "year": 2015,
          "name": "Seq2Seq with LSTM"
        }
      ]
    }
  },
  "back": {
    "layout": "auto",
    "text": "Long Short-Term Memory (LSTM) networks, introduced by Hochreiter and Schmidhuber in 1997, represent a specialized form of recurrent neural networks designed to overcome the vanishing gradient problem that plagued traditional RNNs. LSTMs revolutionized sequence modeling by enabling neural networks to learn and remember patterns over extended sequences of data, making them particularly effective for tasks requiring temporal context.\n\nThe LSTM architecture incorporates memory cells with carefully regulated information flow controlled by three gates: an input gate determining what new information to store, a forget gate deciding what information to discard, and an output gate controlling what information to pass to the next layer. This gating mechanism allows LSTMs to selectively remember or forget information, making them adept at capturing long-range dependencies in sequential data. Before transformers became dominant, LSTMs were the backbone of state-of-the-art systems in machine translation, speech recognition, text generation, and time series prediction. Even with the rise of attention-based models, LSTMs remain valuable for many sequence modeling tasks, especially those with limited data or computational resources."
  },
  "meta": {
    "modified": "2026-02-06T09:23:32.759Z",
    "migratedFrom": "json",
    "categories": [
      "LLMmodels"
    ]
  }
}