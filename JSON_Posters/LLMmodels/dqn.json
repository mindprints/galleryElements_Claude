{
  "uid": "dqn",
  "header": "Deep Q-Networks (DQN), introduced by DeepMind in 2013, revolutionized reinforcement learning by successfully combining deep neural networks with Q-learning algorithms to solve complex control problems. This breakthrough architecture addressed the fundamental challenge of learning effective policies directly from high-dimensional sensory inputs, such as pixels in video games, without requiring hand-engineered features or domain-specific knowledge.\n\nThe key innovation of DQN lies in its stable training methodology, which overcame the notorious instability issues of previous attempts to use neural networks for reinforcement learning. Two critical components enabled this stability: experience replay, which stores and randomly samples past experiences to break correlations in the training data, and a separate target network that slowly tracks the main network to reduce oscillations in the learning process. DQN gained worldwide attention when it mastered a diverse set of Atari 2600 games at human or superhuman levels using only raw pixel input and game scores. This achievement demonstrated the potential for end-to-end reinforcement learning systems to develop sophisticated behaviors without explicit programming. DQN's success sparked a revolution in deep reinforcement learning, inspiring numerous extensions and improvements including Double DQN, Dueling DQN, and Prioritized Experience Replay, laying the groundwork for subsequent milestones like AlphaGo and more advanced reinforcement learning algorithms.",
  "figure": "Deep Q-Network",
  "chronology": {
    "epochStart": 2013,
    "epochEvents": [
      { "year": 2013, "name": "Original DQN Paper" },
      { "year": 2015, "name": "Nature Paper on Atari Games" },
      { "year": 2016, "name": "Double DQN" },
      { "year": 2016, "name": "Dueling Network Architecture" },
      { "year": 2017, "name": "Rainbow DQN" }
    ],
    "epochEnd": 2020
  }
} 