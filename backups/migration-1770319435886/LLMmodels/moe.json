{
  "uid": "moe",
  "header": "Mixture of Experts (MoE) is a neural network architecture that embodies the principle of divide-and-conquer in machine learning by combining multiple specialized neural networks (\"experts\") with a gating mechanism that dynamically routes inputs to the most appropriate experts. This approach contrasts with traditional neural networks where all inputs are processed by the entire network, regardless of their characteristics or difficulty level.\n\nThe key innovation of MoE models lies in their conditional computation paradigm, which enables efficient scaling of model capacity without proportionally increasing computational costs. In modern implementations, such as those used in large language models, each token is processed by only a small subset of the model's parameters, selected by a learned routing function. This sparse activation pattern allows MoE models to achieve remarkable parameter efficiency, effectively decoupling model size from computational cost. Models like Google's Switch Transformer and Meta's Mixtral demonstrated that MoE architectures can significantly outperform dense models of comparable computational cost, or match the performance of much larger dense models while using far less computation per token. As AI systems continue to scale, MoE approaches offer a promising path toward more efficient and capable models, enabling continued growth in model capacity while managing computational requirements.",
  "figure": "Mixture of Experts",
  "chronology": {
    "epochStart": 1991,
    "epochEvents": [
      { "year": 1991, "name": "Original MoE Concept (Jacobs et al.)" },
      { "year": 2017, "name": "Sparsely-Gated MoE Layers" },
      { "year": 2021, "name": "Switch Transformer" },
      { "year": 2022, "name": "GShard and GLaM Models" },
      { "year": 2023, "name": "Mixtral 8x7B Open Release" }
    ],
    "epochEnd": 2024
  }
} 