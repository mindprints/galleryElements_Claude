{
  "uid": "attention",
  "header": "Attention mechanisms transformed deep learning by enabling models to focus selectively on specific parts of input data, mimicking human cognitive attention. First introduced in the context of neural machine translation in 2014, attention allows a model to dynamically weight the importance of different elements in a sequence when generating each output element, rather than compressing all information into a fixed-length vector.\n\nThe key innovation of attention lies in its ability to create direct connections between input and output elements regardless of their distance in a sequence, effectively addressing the limitations of recurrent neural networks in handling long-range dependencies. The breakthrough self-attention mechanism, which relates different positions of a single sequence to compute a representation, became the cornerstone of the Transformer architecture in 2017. Various forms of attention have since been developed, including additive, multiplicative, scaled dot-product, and multi-head attention. Attention mechanisms have proven crucial not only for natural language processing but also for computer vision, speech recognition, and multimodal learning tasks.",
  "figure": "Attention",
  "chronology": {
    "epochStart": 2014,
    "epochEvents": [
      { "year": 2014, "name": "Neural Machine Translation" },
      { "year": 2015, "name": "Attention in Image Captioning" },
      { "year": 2016, "name": "Neural Turing Machines" },
      { "year": 2017, "name": "Self-Attention in Transformers" },
      { "year": 2019, "name": "Sparse Attention Mechanisms" }
    ],
    "epochEnd": 2020
  }
} 